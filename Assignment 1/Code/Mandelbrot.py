# -*- coding: utf-8 -*-
"""Mandelbrot_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AlyxTPwzV2uc8N-wyxJCJ8WvCHhn1cTy

## Define a function to get samples
"""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats.distributions import uniform
import pandas as pd
from google.colab import drive
import os
import random
import time
import scipy as sc
from bridson import poisson_disc_samples
import chaospy

text_size = 25
line_width = 2.5

plt.style.use('ggplot')
plt.rc('xtick', labelsize=20) 
plt.rc('ytick', labelsize=20) 

# Mount your Drive to the Colab VM.
drive.mount('/drive')
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1"
path_figures = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/figures"

def get_sample(s, method = "random", return_time = False, r = 0.0331):
  '''returns an s number of complex samples with selected method (does not apply to orthogonal
  and Poisson disk sampling). Number of samples returned for orthogonal will be the
  (int(s**0.5))**2 and for Poisson disk sampling the number of samples depend on r parameter.'''
  # set boundaries of the search plane
  start = time.perf_counter()
  x_lower = -2.2
  x_upper = 0.55
  y_lower = -1.2
  y_upper = 1.2
  if method == "random":
    x = np.random.uniform(x_lower, x_upper, s)
    y = np.random.uniform(y_lower, y_upper, s)
  if method == "hypercube":
    x_lower_bounds = np.linspace(x_lower, x_upper, s+1)[0:s]
    x_upper_bounds = np.linspace(x_lower, x_upper, s+1)[1:s+1]
    y_lower_bounds = np.linspace(y_lower, y_upper, s+1)[0:s]
    y_upper_bounds = np.linspace(y_lower, y_upper, s+1)[1:s+1]
    samples_x = np.random.uniform(low=x_lower_bounds, high = x_upper_bounds, size = [1,s]).T
    samples_y = np.random.uniform(low=y_lower_bounds, high = y_upper_bounds, size = [1,s]).T
    samples = np.hstack((samples_x, samples_y))
    np.random.shuffle(samples[:,1])
    x = samples[:, 0]
    y = samples[:, 1]
  if method == "orthogonal":
    # found at and adjusted from: https://codereview.stackexchange.com/questions/207610/orthogonal-sampling
    n = int(np.sqrt(s))
    # Create a list of coordinates of major grid with subcoordinate of minor grids in the major grid
    grids = {(i,j):[(a,b) for a in range(n) for b in range(n)] for i in range(n) for j in range(n)}
    samples = []
    append = samples.append
    for grid in grids: # iterate over major grids
      sample = random.choice(grids[grid]) # choose a random minor grid where the samples will be placed inside the current major grid
      # evaluate which other minor grids the major grid intercepts with  
      lst_row = [(k1, b) for (k1, b), v in grids.items() if k1 == grid[0]]
      lst_col = [(a, k1) for (a, k1), v in grids.items() if k1 == grid[1]]

      # remove the subgrids that would vialote the latin hypercube requiremets in regards to the current sample selected
      for col in lst_col:
        grids[col] = [a for a in grids[col] if a[1] != sample[1]]
      for row in lst_row:
        grids[row] = [a for a in grids[row] if a[0] != sample[0]]
      #Adjust the sample to fit the grid it falls in  
      sample = (sample[0] + n * grid[0], sample[1] + n * grid[1])
      append(sample)
      
    # scale the given orthogonal samples to the search space required
    scalar_x = (np.abs(x_lower) + np.abs(x_upper))/len(samples)
    scalar_y = (np.abs(y_lower) + np.abs(y_upper))/len(samples)
    scaled_points = [(-2.2 + scalar_x*p[0], -1.12 + scalar_y*p[1]) for p in samples]
    x = [np.random.uniform(p[0], p[0]+scalar_x) for p in scaled_points]
    y = [np.random.uniform(p[1], p[1]+scalar_x) for p in scaled_points]

  if method == "hammersley":
    distribution = chaospy.J(chaospy.Uniform(x_lower, x_upper), chaospy.Uniform(y_lower, y_upper))
    samples = distribution.sample(s, rule="korobov")
    x_regular = samples[0] 
    y_regular = samples[1]

    min_noise = -0.05
    max_noise = 0.05
    x = [x + np.random.uniform(min_noise, max_noise) for x in x_regular]
    y = [y + np.random.uniform(min_noise, max_noise) for y in y_regular]

  if method == "pds":
    sample = poisson_disc_samples(width=(2.2+0.55), height=(1.2*2), r=r)
    x = [s[0] - 2.2 for s in sample]
    y = [s[1] - 1.2 for s in sample]

  # turn samples into complex numbers
  z_list =[complex(x, y) for x, y in zip(x, y)]
  end = time.perf_counter()
  time_taken = end - start
  if return_time == True:
    return z_list, time_taken
  else:
    return z_list

def give_anti_random(s, return_time = False):
  '''returns two random samples of size n - one regular and one antithetic'''
  x_lower = -2.2
  x_upper = 0.55
  y_lower = -1.2
  y_upper = 1.2 
  middle_x = (x_lower + x_upper)/2
  middle_y = (y_lower + y_upper)/2
  start_reg = time.perf_counter()
  x = np.random.uniform(x_lower, x_upper, s)
  y = np.random.uniform(y_lower, y_upper, s)
  z_list_reg = [complex(x, y) for x, y in zip(x, y)]
  end_reg = time.perf_counter()
  start_anti = time.perf_counter()
  x_anti = [x - 2 * (x - middle_x) for x in x]
  y_anti = [y - 2 * (y - middle_y) for y in y]
  z_list_anti = [complex(x, y) for x, y in zip(x_anti, y_anti)]
  end_anti = time.perf_counter()
  time_taken_reg = end_reg - start_reg
  time_taken_anti = end_anti - start_anti
  if return_time == True:
    return z_list_reg, z_list_anti, time_taken_reg, time_taken_anti 
  return z_list_reg, z_list_anti

"""## Define function to run monte carlo with the samples list"""

def f_iterations(c, iterations):
  ''' performs manderlbrot iterations and returns number of itterations
  before the real part of the complex number reached 2'''
    z=c
    i=1
    while i<iterations and z.real<2:
        z=z*z +c
        i+=1
    return i

def f_iterations_time(c, iterations):
    ''' performs manderlbrot iterations and returns number of itterations
  before the real part of the complex number reached 2, also returns time taken for itterations'''
    start = time.perf_counter()
    z=c
    i=1
    while i<iterations and z.real<2:
        z=z*z +c
        i+=1
    end = time.perf_counter()
    time_taken = end - start
    return i, time_taken

def get_area(z_list, i, return_time = False):
      '''estimates area of the mandelbrot set via Monte Carlo integration'''
  # set boundaries of the search space
  x_lower = -2.2
  x_upper = 0.55
  y_lower = -1.2
  y_upper = 1.2
  # calculate search space area
  area = (np.abs(x_lower) + np.abs(x_upper)) * (np.abs(y_lower) + np.abs(y_upper))
  # perform itterations
  if return_time == False:
    i_inside = sum([f_iterations(z, i) == i for z in z_list])
    return i_inside/len(z_list) * area
  else:
    i_inside = 0
    time_elapsed = 0
    for z in z_list:
      iterations, time_taken = f_iterations_time(z, i)
      if iterations == i:
        i_inside += 1
      time_elapsed += time_taken
    return i_inside/len(z_list) * area, time_elapsed

"""## Define a fucntion for reading simulations data from a directory"""

def read_files_dir(path):
  '''function for reading and combining files in a directory'''
  os.chdir(path)
  file_list = os.listdir()
  df_1 = pd.DataFrame()
  for i in file_list:
    if i[len(i)-3:len(i)] != "csv":
      continue
    print(i)
    data = pd.read_csv(i)
    data["file"] = i
    df_1 = pd.concat([df_1, data])
  return df_1

"""# 1.Plot the Manderlbrot set"""

y_steps = 1000
x_steps = int(y_steps*1.1)
 
x = np.linspace(-2, 0.5, num=x_steps)
y = np.linspace(-1.12, 1.12, num=y_steps)

iterations = np.zeros((x_steps, y_steps))

i = 1000
xx = 0
for x_1 in x:
  yy = 0
  for y_1 in y:
    iterations[xx, yy] = get_iter(complex(x_1, y_1), iterations = i)
    yy += 1
  xx += 1

plt.figure(figsize=(20,10))
plt.imshow(iterations.T)
plt.axis("off")
plt.show()

"""#2. Finding s and i

##2.1 Pure random sampling

### 2.1.1 Setting constant s and changing i. Keeping the sample the same in all runs of i.
"""

start = 50
end = 10000
steps = 500
i_list = np.linspace(start, end, steps)
simulations = 10
df = pd.DataFrame()
for sim in range(simulations):
  sample = get_sample(10000)
  areas = []
  for i in i_list:
    print(i)
    areas.append(get_area(sample, int(i)))
  df_dict = {"sim": sim,
            "i": [int(i) for i in i_list],
            "area": areas} 
  df_i = pd.DataFrame(df_dict)
  df = df.append(df_i)
#df.to_csv(path + "/results s10000,i_start50,i_end10000,steps500, sim3_1.csv")

"""#### 2.1.1.1 Read the simulation data"""

path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing i"
sim = read_files_dir(path)

"""#### 2.1.1.2 Plot results of the simulations, set threshold for the error and find the i"""

# find the best estimate of each run and join to the main data
min_areas_per_sim = sim.groupby(["file", "sim"])["area"].min().reset_index().rename(columns={'area':'min_area'})
sim = sim.merge(min_areas_per_sim, how = "left", on = ["file", "sim"])
# calculate the errror for each i in simulations
sim["error"] = sim["area"] - sim["min_area"]
# calculate the average error for each i
average_errors = sim.groupby("i")["error"].mean().reset_index()

# set error threshold to 0.001
threshold = 0.001
plt.figure(figsize=(14,7))
plt.hlines(threshold, xmin = 0, xmax = 10000, linestyle='--', color = "grey", label = "Threshold", lw = line_width)
plt.plot(average_errors["i"], average_errors["error"], lw=line_width)
chosen_i = average_errors[average_errors.error<=threshold].iloc[0,:]["i"]
plt.scatter(chosen_i, threshold, s = 100, c = 'black', zorder = 10, label = "Corresponding $i$")
print(chosen_i)
plt.ylabel("Average error", size = text_size)
plt.xlabel("$i$", size = text_size)
plt.legend(loc = "best", prop={'size': text_size})
plt.xticks(fontsize = text_size)
plt.yticks(fontsize = text_size)
plt.savefig(path_figures + "/finding_i.png", dpi = 300)

# caluclate 95 percent confidence interval for chosen i
samples = 50
sd = np.std(sim[sim.i == chosen_i].area)
print(threshold+1.96*sd/np.sqrt(samples),threshold-1.96*sd/np.sqrt(samples))

"""### 2.1.2 Setting constant i = 2582 and changing s."""

i = chosen_i
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s"
start = 50
end = 10000
steps = 500
s_list = np.linspace(start, end, steps)
simulations = 50
df = pd.DataFrame()
for sim in range(simulations):
  areas = []
  times = []
  print(sim)
  for s in s_list:
    sample = get_sample(int(s))
    area, time_taken = get_area(sample, int(i), return_time = True)
    areas.append(area)
    times.append(time_taken)
  df_dict = {"sim": sim,
             "i": i,
            "s": [int(s) for s in s_list],
            "area": areas,
             "time": times} 
  df_i = pd.DataFrame(df_dict)
  df = df.append(df_i)
#df.to_csv(path + "/balancing s results i2582,s_start50,s_end10000,steps500 sim50.csv")
print(df)

"""#### 2.1.2.1 Cambining the resaults of the simulations and finding the s with std < 0.05"""

path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s"
sim_s = read_files_dir(path)
std_data = sim_s.groupby("s")["area"].std().reset_index()
plt.plot(std_data["s"], std_data["area"])

# define an exponential curve and fit it to the data

def func(x, a, b):
    return x**(a)*b

np.random.seed(1700)
popt_random, pcov = sc.optimize.curve_fit(func, std_data.s, std_data.area)
plt.figure(figsize=(14,7))
plt.plot(std_data["s"], std_data["area"], lw = line_width)
plt.plot(std_data.s, func(std_data.s, *popt_random), 'b-', lw = line_width, label = "Exponential fit")
plt.ylabel("Mean sample std", size = text_size)
plt.xlabel("Samples", size = text_size)
plt.legend(loc = "best")
plt.legend(loc = "best", prop={'size': text_size})
plt.xticks(fontsize = text_size)
plt.yticks(fontsize = text_size)
plt.savefig(path_figures + "/finding_s_random.png", dpi = 300)

start = 50
end = 10000
steps = 500
s_list = np.linspace(start, end, steps)
### find where function reaches 0.05
for s in s_list:
  std = func(int(s), *popt_random)
  if std < 0.05:
    chosen_s = int(s)
    break
print(chosen_s)
print("we need to select 3025 for orthogonal")

import time

start_time = time.time()

get_area(get_sample(5000, method="orthogonal"), 2582)

end_time = time.time()
print(f"It took {end_time-start_time:.2f} seconds to compute")

"""##2.2 Latin hyper cube sampling

### 2.2.1 Setting constant i = 2582 and changing s.
"""

i = 2582
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s/hypercube"
start = 50
end = 10000
steps = 500
s_list = np.linspace(start, end, steps)
simulations = 50
df = pd.DataFrame()
for sim in range(simulations):
  areas = []
  times = []
  print(sim)
  for s in s_list:
    sample = get_sample(int(s), method = "hypercube")
    area, time_taken = get_area(sample, int(i), return_time = True)
    areas.append(area)
    times.append(time_taken)
  df_dict = {"sim": sim,
             "i": i,
            "s": [int(s) for s in s_list],
            "area": areas,
             "time": times} 
  df_i = pd.DataFrame(df_dict)
  df = df.append(df_i)
#df.to_csv(path + "/balancing s results i_2582,s_start50,s_end10000,steps500 sim50.csv")
print(df)

"""#### 2.2.1.1 Cambining the results of the simulations and finding the s with std < 0.05"""

path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s/hypercube"
sim_s = read_files_dir(path)
std_data_hyper = sim_s.groupby("s")["area"].std().reset_index()
plt.plot(std_data_hyper["s"], std_data_hyper["area"])

# define an exponential curve and fit it to the data

def func(x, a, b):
    return x**(a)*b

np.random.seed(1700)
popt_hyper, pcov = sc.optimize.curve_fit(func, std_data_hyper.s, std_data_hyper.area)
plt.plot(std_data_hyper["s"], std_data_hyper["area"])
plt.plot(std_data_hyper.s, func(std_data_hyper.s, *popt_hyper), 'r-')
print(*popt)

start = 50
end = 10000
steps = 500
s_list = np.linspace(start, end, steps)
### find where function reaches 0.05
for s in s_list:
  std = func(int(s), *popt_hyper)
  if std < 0.05:
    chosen_s_hyper = int(s)
    break
print(chosen_s_hyper)

"""## 2.3 Orthogonal sampling

### 2.3.1 setting constant i = 2582 and changing s
"""

import math
i = 2582
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s/orthogonal"
start = 50
end = 3000
steps = 150
s_list = np.linspace(start, end, steps)
s_list_adjusted = [int(math.sqrt(s))**2 for s in s_list] 
# remove duplicate
s_list_no_duplicates = []
for x in s_list_adjusted:
    if x not in s_list_no_duplicates:
        s_list_no_duplicates.append(x)

simulations = 10
df = pd.DataFrame()
for sim in range(simulations):
  areas = []
  times = []
  print(sim)
  for s in s_list_no_duplicates:
    sample = get_sample(int(s), method = "orthogonal")
    area, time_taken = get_area(sample, int(i), return_time = True)
    areas.append(area)
    times.append(time_taken)
  df_dict = {"sim": sim,
             "i": i,
            "s": [s for s in s_list_no_duplicates],
            "area": areas,
             "time": times} 
  df_i = pd.DataFrame(df_dict)
  df = df.append(df_i)
df.to_csv(path + "/balancing s results i2582,s_start50,s_end10000,steps500 sim10_1.csv")
print(df)

"""#### 2.3.1.1 Cambining the results of the simulations and finding the s with std < 0.05"""

path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/changing s/orthogonal"
sim_s = read_files_dir(path)
std_data_orth = sim_s.groupby("s")["area"].std().reset_index()
plt.plot(std_data_orth["s"], std_data_orth["area"])
len(std_data_orth["s"])

# define an exponential curve and fit it to the data
start = 50
end = 10000
steps = 500


def func(x, a, b):
    return x**(a)*b

np.random.seed(1700)
popt_orth, pcov = sc.optimize.curve_fit(func, std_data_orth.s, std_data_orth.area)
plt.plot(std_data_orth["s"], std_data_orth["area"])
plt.plot(np.linspace(start, end, steps), func(np.linspace(start, end, steps), *popt_orth), 'b-')

### find where function reaches 0.05
for s in s_list_no_duplicates:
  std = func(int(s), *popt_orth)
  if std < 0.05:
    chosen_s_orth = int(s)
    print(std)
    break
print(chosen_s_orth)

"""## 2.4 Combining all of the results for finding s into one plot"""

### combining all three plots
std_threshold = 0.05
fig, axs = plt.subplots(3, sharex=True)
fig.set_figheight(15)
fig.set_figwidth(15)
axs[0].plot(std_data["s"], std_data["area"], lw = line_width)
axs[0].plot(np.linspace(start, end, steps), func(np.linspace(start, end, steps), *popt_random), 'b-', lw = line_width, label = "Exponential fit")
axs[0].hlines(std_threshold, xmin = 0, xmax = 10000, linestyle='--', color = "grey", label = "Threshold", lw = line_width)
axs[0].scatter(chosen_s, std_threshold, s = 100, c = 'black', zorder = 10, label = "Corresponding $s$")
axs[0].legend(loc = "best", prop={'size': text_size})
axs[0].set_title('Random', size = text_size)
axs[1].plot(std_data_hyper["s"], std_data_hyper["area"], lw = line_width)
axs[1].plot(np.linspace(start, end, steps), func(np.linspace(start, end, steps), *popt_hyper), 'b-', lw = line_width)
axs[1].scatter(chosen_s_hyper, std_threshold, s = 100, c = 'black', zorder = 10)
axs[1].hlines(std_threshold, xmin = 0, xmax = 10000, linestyle='--', color = "grey", lw = line_width)
axs[1].legend(loc = "best", prop={'size': text_size})
axs[1].set_title('Latin hypercube', size = text_size)
axs[2].plot(std_data_orth["s"], std_data_orth["area"], lw = line_width)
axs[2].plot(np.linspace(start, end, steps), func(np.linspace(start, end, steps), *popt_orth), 'b-', lw = line_width)
axs[2].scatter(chosen_s_orth, std_threshold, s = 100, c = 'black', zorder = 10)
axs[2].hlines(std_threshold, xmin = 0, xmax = 10000, linestyle='--', color = "grey", lw = line_width)
axs[2].legend(loc = "best", prop={'size': text_size})
axs[2].set_title('Orthogonal', size = text_size)
plt.subplots_adjust(hspace = 0.15)
fig.text(0.5, 0.08, '$s$', ha='center', size = text_size)
fig.text(0.05, 0.5, 'Sample standard deviation', va='center', rotation='vertical', size = text_size)
plt.savefig(path_figures + "/finding_s.png", dpi = 300)

"""# 3. Running multiple simulations with i = 2582 and s = 3025 and checking their quality with CI

## 4.1 Random, hypercube, orthogonal
"""

### run 100 simulations for random, latin hypercube and orthogonal
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i"
simulations = 100
i = 2582
s = 3025
df = pd.DataFrame()
area_random = []
times_random = []
area_hypercube = []
times_hypercube = []
area_orthogonal = []
times_orthogonal = []
for sim in range(simulations):
    print(sim)
    # random
    sample, time_sample = get_sample(int(s), method = "random", return_time = True)
    area, time_taken = get_area(sample, int(i), return_time = True)
    area_random.append(area)
    times_random.append(time_taken+time_sample)
    # hypercube
    sample, time_sample = get_sample(int(s), method = "hypercube", return_time = True)
    area, time_taken = get_area(sample, int(i), return_time = True)
    area_hypercube.append(area)
    times_hypercube.append(time_taken+time_sample)
    # random
    sample, time_sample = get_sample(int(s), method = "orthogonal", return_time = True)
    area, time_taken = get_area(sample, int(i), return_time = True)
    area_orthogonal.append(area)
    times_orthogonal.append(time_taken+time_sample)
df_dict = {"sim": np.linspace(0,99, 100),
          "i": i,
          "s": s,
          "area_random": area_random,
          "area_hypercube": area_hypercube,
          "area_orthogonal": area_orthogonal, 
          "times_random": times_random,
          "times_hypercube": times_hypercube,
          "times_orthogonal": times_orthogonal}  

df = pd.DataFrame(df_dict)
#df.to_csv(path + "/results.csv")

# get statistics of the simulations
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i"
sim_100 = read_files_dir(path)
print(sim_100.mean())
print(sim_100.std())

# calculate moving std and then CI
sim_100["std_random"]= sim_100["area_random"].expanding(5).std()
sim_100["ci_random"]= 1.96*sim_100["std_random"]/(sim_100["sim"]**0.5)
sim_100["std_hypercube"]= sim_100["area_hypercube"].expanding(5).std()
sim_100["ci_hypercube"]= 1.96*sim_100["std_hypercube"]/(sim_100["sim"]**0.5)
sim_100["std_orthogonal"]= sim_100["area_orthogonal"].expanding(5).std()
sim_100["ci_orthogonal"]= 1.96*sim_100["std_orthogonal"]/(sim_100["sim"]**0.5)
plt.figure(figsize=(14,7))
plt.plot(sim_100.sim+1, sim_100.ci_random, label = "Random", lw=line_width)
plt.plot(sim_100.sim+1, sim_100.ci_hypercube, label = "Latin hypercube", lw=line_width)
plt.plot(sim_100.sim+1, sim_100.ci_orthogonal, label = "Orthogonal", lw=line_width)
plt.legend(loc = "best", prop={'size': text_size})
plt.xlabel("Simulations", size = text_size)
plt.ylabel("Radius of CI (95%)", size = text_size)
plt.legend(loc = "best", prop={'size': text_size})

"""##4.1.1 Poisson disk sampling"""

### run 100 simulations for poisson disk sampling
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i/pds"
simulations = 100
i = 2582
s = 3025
df = pd.DataFrame()
area_pds = []
times_pds = []
for sim in range(simulations):
    print(sim)
    # pds
    sample, time_sample = get_sample(int(s), method = "pds", return_time = True)
    area, time_taken = get_area(sample, int(i), return_time = True)
    area_pds.append(area)
    times_pds.append(time_taken+time_sample)
df_dict = {"sim": np.linspace(0,99, 100),
          "i": i,
          "s": s,
          "area_pds": area_pds,
          "times_pds": times_pds}  

df = pd.DataFrame(df_dict)
df.to_csv(path + "/results_pds.csv")

# read simulations results calculate moving std and CI
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i/pds"
sim_100_pds = read_files_dir(path)
print(sim_100_pds.mean())
print(sim_100_pds.std())
plt.figure(figsize=(14,7))
sim_100_pds["std_pds"]= sim_100_pds["area_pds"].expanding(5).std()
sim_100_pds["ci_pds"]= 1.96*sim_100_pds["std_pds"]/(sim_100_pds["sim"]**0.5)

"""## 4.1.2 Hammersley"""

### run 100 simulations for randomised hammersley
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i/qms"
simulations = 100
i = 2582
s = 3025
df = pd.DataFrame()
areas = []
times = []
for sim in range(simulations):
    print(sim)
    # pds
    sample, time_sample = get_sample(int(s), method = "hammersley", return_time = True)
    area, time_taken = get_area(sample, int(i), return_time = True)
    areas.append(area)
    times.append(time_taken+time_sample)
df_dict = {"sim": np.linspace(0,99, 100),
          "i": i,
          "s": s,
          "area_h": areas,
          "times_h": times}  

df = pd.DataFrame(df_dict)
df.to_csv(path + "/results_hammersley.csv")

# read simulations results calculate moving std and CI
path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i/qms"
sim_100_h = read_files_dir(path)
print(sim_100_h.mean())
print(sim_100_h.std())
plt.figure(figsize=(14,7))
sim_100_h["std_h"]= sim_100_h["area_h"].expanding(5).std()
sim_100_h["ci_h"]= 1.96*sim_100_h["std_h"]/(sim_100_h["sim"]**0.5)

"""### 4.1.1.1 t-test between pds, Hammersley and orthogonal sampling"""

from scipy import stats
print("Welch test between pds and orthogonal:")
print(stats.ttest_ind(np.array(sim_100["area_orthogonal"]), np.array(sim_100_pds.area_pds), equal_var = False))
print("Welch test between Hammersley and orthogonal:")
print(stats.ttest_ind(np.array(sim_100["area_orthogonal"]), np.array(sim_100_h["area_h"]), equal_var = False))

"""## 4.2 Antithetic sampling"""

path = "/drive/My Drive/UvA/Stochastic Simulations/Assignment 1/100 simulations with same s and i/antithetic"
simulations = 50
i = 2582
s = 3025
df = pd.DataFrame()
areas_anti = []
areas_time = []
for sim in range(simulations):
    print(sim)
    # pds
    reg, anti, time_reg, time_anti = give_anti_random(s, return_time=True)
    area_reg, time_area_reg = get_area(reg, i, return_time = True)
    area_anti, time_area_anti = get_area(anti, i, return_time = True)
    areas_anti.append(area_reg)
    areas_time.append(time_reg+time_area_reg)
    areas_anti.append(area_anti)
    areas_time.append(time_anti+time_area_anti)

df_dict = {"sim": np.linspace(0,99, 100),
          "i": i,
          "s": s,
          "area_anti": areas_anti,
           "time": areas_time}  

df = pd.DataFrame(df_dict)
#df.to_csv(path + "/results_anti.csv")

"""## 4.4.1 Plot extra methods + orthogonal"""

plt.figure(figsize=(14,7))
plt.plot(sim_100_pds.sim+1, sim_100_pds.ci_pds, label = "Poisson disk", lw=line_width)
plt.plot(sim_100_anti.sim+1, sim_100_anti.ci_anti, label = "Antithetic", lw=line_width)
plt.plot(sim_100.sim+1, sim_100.ci_orthogonal, label = "Orthogonal", lw=line_width)
plt.plot(sim_100_h.sim+1, sim_100_h.ci_h, label = "Hammersley", lw = line_width)
plt.legend(loc = "best", prop={'size': text_size})
plt.xlabel("Simulations", size = text_size)
plt.ylabel("Radius of CI (95%)", size = text_size)
plt.legend(loc = "best", prop={'size': text_size})